{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:25:08.959643Z",
     "start_time": "2024-10-18T05:25:05.394115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, Sized\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "from torchvision.transforms.functional import resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from helpers.datasets import custom_collate_proposals_fn, CrackDatasetForClassificationWithProposals\n",
    "from helpers.early_stopping import EarlyStopping"
   ],
   "id": "f5c1702b2f26f516",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:25:08.967990Z",
     "start_time": "2024-10-18T05:25:08.961648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Resnext50BasedClassifierForProposals(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_shape: tuple[int, int, int] = (3, 224, 224),\n",
    "            linear_layers_features: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.DEFAULT)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 1)\n",
    "        )\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _get_feature_size(self, shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "\n",
    "            return features.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        class_scores = self.classifier(features)\n",
    "\n",
    "        return class_scores"
   ],
   "id": "c45af1d6db4f9511",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:25:08.992449Z",
     "start_time": "2024-10-18T05:25:08.968995Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "a222ee1e673144a3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:25:08.999657Z",
     "start_time": "2024-10-18T05:25:08.993455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "proposal_cache = defaultdict(list)\n",
    "SELECTIVE_SEARCH_BATCH_SIZE = 200\n",
    "\n",
    "\n",
    "def perform_selective_search(image: np.ndarray, image_path: str) -> torch.Tensor:\n",
    "    if image_path in proposal_cache:\n",
    "        random.shuffle(proposal_cache[image_path])\n",
    "        \n",
    "        return torch.tensor(proposal_cache[image_path], dtype=torch.float32)\n",
    "    else:\n",
    "        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "        ss.setBaseImage(image)\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "\n",
    "        rects = ss.process()\n",
    "        boxes = []\n",
    "\n",
    "        for (x, y, w, h) in rects:\n",
    "            area = w * h\n",
    "            \n",
    "            boxes.append((x, y, x + w, y + h, area))\n",
    "\n",
    "        boxes = sorted(boxes, key=lambda b: b[4], reverse=True)\n",
    "        boxes = [(x1, y1, x2, y2) for x1, y1, x2, y2, area in boxes]\n",
    "\n",
    "        random.shuffle(boxes)\n",
    "\n",
    "        proposal_cache[image_path] = boxes\n",
    "\n",
    "        return torch.tensor(boxes, dtype=torch.float32)"
   ],
   "id": "1d205e04b56026b8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:25:09.005895Z",
     "start_time": "2024-10-18T05:25:09.000662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_loaders() -> tuple[DataLoader, DataLoader]:\n",
    "    train_images_dir = os.path.join(\"data\", \"train\", \"images\")\n",
    "    valid_images_dir = os.path.join(\"data\", \"valid\", \"images\")\n",
    "    train_coco_path = os.path.join(\"data\", \"train\", \"coco_annotations.json\")\n",
    "    valid_coco_path = os.path.join(\"data\", \"valid\", \"coco_annotations.json\")\n",
    "    train_dataset = CrackDatasetForClassificationWithProposals(\n",
    "        perform_selective_search,\n",
    "        train_coco_path,\n",
    "        train_images_dir\n",
    "    )\n",
    "    valid_dataset = CrackDatasetForClassificationWithProposals(\n",
    "        perform_selective_search,\n",
    "        valid_coco_path,\n",
    "        valid_images_dir\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_proposals_fn\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_proposals_fn\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ],
   "id": "a3079d112419fc1e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:25:09.012095Z",
     "start_time": "2024-10-18T05:25:09.007901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_loop_objects() -> tuple[Resnext50BasedClassifierForProposals, EarlyStopping, torch.nn.BCEWithLogitsLoss, optim.Adam]:\n",
    "    model = Resnext50BasedClassifierForProposals()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, delta=0)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    return model, early_stopping, criterion, optimizer"
   ],
   "id": "22919126908270e6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T14:17:34.594188Z",
     "start_time": "2024-10-18T05:25:09.013102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 25\n",
    "checkpoint_path = os.path.join(\"checkpoints\", f\"resnext50_32x4d_classifier_with_proposals.pt\")\n",
    "train_loader, valid_loader = get_loaders()\n",
    "model, early_stopping, criterion, optimizer = get_loop_objects()\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": []\n",
    "}\n",
    "valid_accuracy = 0\n",
    "\n",
    "\n",
    "def chunkify(x: Iterable | Sized) -> Iterable[Iterable]:\n",
    "    for i in range(0, len(x), SELECTIVE_SEARCH_BATCH_SIZE):\n",
    "        yield x[i:i + SELECTIVE_SEARCH_BATCH_SIZE]\n",
    "\n",
    "\n",
    "def process_and_calculate_loss(\n",
    "        image: torch.Tensor, \n",
    "        proposals: torch.Tensor, \n",
    "        labels: Iterable[int]\n",
    ") -> tuple[torch.Tensor, int, int]:\n",
    "    cropped_proposals_with_labels = []\n",
    "\n",
    "    for idx, proposal in enumerate(proposals):\n",
    "        label = labels[idx]\n",
    "        x_min, y_min, x_max, y_max = proposal.int()\n",
    "        cropped_region = image[:, y_min:y_max, x_min:x_max]\n",
    "        resized_region = resize(cropped_region, [224, 224])\n",
    "\n",
    "        cropped_proposals_with_labels.append((label, resized_region))\n",
    "\n",
    "    batch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for chunk in chunkify(cropped_proposals_with_labels):\n",
    "        labels_batch, proposals_batch = zip(*chunk)\n",
    "        labels_batch = torch.stack(labels_batch).to(device)\n",
    "        regions = torch.stack(proposals_batch).to(device)\n",
    "        predictions = model(regions).squeeze(1)\n",
    "        loss = criterion(predictions, labels_batch)\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "        if model.training:\n",
    "            loss.backward()\n",
    "\n",
    "        predicted = (predictions > 0.5).float()\n",
    "        correct += (predicted == labels_batch).sum().item()\n",
    "        total += labels_batch.numel()\n",
    "\n",
    "    return batch_loss, correct, total\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    epoch_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for images_group, proposals_group, labels_group in epoch_progress:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for image, proposals, labels in zip(images_group, proposals_group, labels_group):\n",
    "            batch_loss, correct, total = process_and_calculate_loss(image, proposals, labels)\n",
    "\n",
    "            train_loss += batch_loss\n",
    "            correct_train += correct\n",
    "            total_train += total\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_progress.set_postfix(loss=train_loss / total_train, accuracy=100. * correct_train / total_train)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images_group, proposals_group, labels_group in valid_loader:\n",
    "            for image, proposals, labels in zip(images_group, proposals_group, labels_group):\n",
    "                batch_loss, correct, total = process_and_calculate_loss(image, proposals, labels)\n",
    "                valid_loss += batch_loss\n",
    "                correct_valid += correct\n",
    "                total_valid += total\n",
    "\n",
    "    valid_accuracy = 100. * correct_valid / total_valid\n",
    "    avg_valid_loss = valid_loss / total_valid\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss / total_train)\n",
    "    history[\"valid_loss\"].append(avg_valid_loss)\n",
    "    print(f\"Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "    early_stopping(avg_valid_loss, model, checkpoint_path)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ],
   "id": "5a0b7e922d4fc39b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 295/295 [2:15:57<00:00, 27.65s/batch, accuracy=85.3, loss=0.00266]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0026, Validation Accuracy: 86.17%\n",
      "Validation loss decreased (inf --> 0.002572).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 295/295 [1:36:21<00:00, 19.60s/batch, accuracy=86.1, loss=0.00245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0027, Validation Accuracy: 86.53%\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 295/295 [1:36:26<00:00, 19.62s/batch, accuracy=86.2, loss=0.00242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0031, Validation Accuracy: 86.36%\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 295/295 [1:36:32<00:00, 19.64s/batch, accuracy=86.4, loss=0.00239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0026, Validation Accuracy: 86.42%\n",
      "Validation loss decreased (0.002572 --> 0.002570).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25:  38%|███▊      | 111/295 [35:21<58:36, 19.11s/batch, accuracy=86.7, loss=0.00238]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 66\u001B[0m\n\u001B[0;32m     63\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m image, proposals, labels \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(images_group, proposals_group, labels_group):\n\u001B[1;32m---> 66\u001B[0m     batch_loss, correct, total \u001B[38;5;241m=\u001B[39m process_and_calculate_loss(image, proposals, labels)\n\u001B[0;32m     68\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_loss\n\u001B[0;32m     69\u001B[0m     correct_train \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m correct\n",
      "Cell \u001B[1;32mIn[7], line 42\u001B[0m, in \u001B[0;36mprocess_and_calculate_loss\u001B[1;34m(image, proposals, labels)\u001B[0m\n\u001B[0;32m     40\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model(regions)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     41\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(predictions, labels_batch)\n\u001B[1;32m---> 42\u001B[0m batch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m     45\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "images_shown = 0\n",
    "should_end = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_group, proposals_group, _ in valid_loader:\n",
    "        if should_end:\n",
    "            break\n",
    "        \n",
    "        for image, proposals in zip(images_group, proposals_group):\n",
    "            if images_shown >= 10:\n",
    "                should_end = True\n",
    "                break\n",
    "\n",
    "            image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "            image_with_boxes = image_np.copy()\n",
    "\n",
    "            for proposal in proposals:\n",
    "                x_min, y_min, x_max, y_max = proposal.int()\n",
    "                x_min = x_min.item()\n",
    "                y_min = y_min.item()\n",
    "                x_max = x_max.item()\n",
    "                y_max = y_max.item()\n",
    "                cropped_region = image[:, y_min:y_max, x_min:x_max].unsqueeze(0).to(device)\n",
    "                prediction = model(cropped_region).cpu().squeeze()\n",
    "                label = \"1\" if prediction > 0.5 else \"0\"\n",
    "                color = (0, 255, 0) if label == \"1\" else (0, 0, 255)\n",
    "\n",
    "                cv2.rectangle(image_with_boxes, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "                cv2.putText(image_with_boxes, label, (x_min, y_min - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Predicted Regions for Image {images_shown+1}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            images_shown += 1"
   ],
   "id": "35f06afcca799ebc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crack_detection_torch_1)",
   "language": "python",
   "name": "crack_detection_torch_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
