{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-15T08:15:43.741089Z",
     "start_time": "2024-10-15T08:15:40.258423Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable\n",
    "\n",
    "from helpers.datasets import CrackDataset, custom_collate_fn\n",
    "from helpers.early_stopping import EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:15:43.756615Z",
     "start_time": "2024-10-15T08:15:43.744097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Resnext50RCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 224, 224), linear_layers_features=512):\n",
    "        super(Resnext50RCNN, self).__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 1)\n",
    "        )\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 4)\n",
    "        )\n",
    "        self.roi_size = (input_shape[1], input_shape[2])\n",
    "\n",
    "    def _get_feature_size(self, shape: tuple):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "            return features.numel()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        class_scores = self.classifier(x)\n",
    "        bbox_deltas = self.bbox_regressor(x)\n",
    "    \n",
    "        return class_scores, bbox_deltas\n",
    "\n",
    "    def extract_region_features(self, image: torch.Tensor, boxes: Iterable[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        For each bounding box, crop the region, resize to the input shape, and pass through the feature extractor.\n",
    "        \"\"\"\n",
    "        regions = []\n",
    "        image = image.float() / 255.0\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.int()\n",
    "            region = image[:, y1:y2, x1:x2]\n",
    "            region_resized = nn.functional.interpolate(region.unsqueeze(0), size=self.roi_size, mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            regions.append(region_resized)\n",
    "\n",
    "        regions = torch.cat(regions, dim=0)\n",
    "        features = self.feature_extractor(regions)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def refine_bboxes(self, proposals: torch.Tensor, deltas: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Refines the original bounding boxes (proposals) using the predicted bbox deltas.\n",
    "        Args:\n",
    "        - proposals: The original bounding boxes from selective search (x1, y1, x2, y2)\n",
    "        - deltas: Predicted bounding box adjustments (dx, dy, dw, dh)\n",
    "\n",
    "        Returns:\n",
    "        - Refined bounding boxes (x1', y1', x2', y2')\n",
    "        \"\"\"\n",
    "        widths = proposals[:, 2] - proposals[:, 0]\n",
    "        heights = proposals[:, 3] - proposals[:, 1]\n",
    "        ctr_x = proposals[:, 0] + 0.5 * widths\n",
    "        ctr_y = proposals[:, 1] + 0.5 * heights\n",
    "        dx = deltas[:, 0]\n",
    "        dy = deltas[:, 1]\n",
    "        dw = deltas[:, 2]\n",
    "        dh = deltas[:, 3]\n",
    "        refined_ctr_x = ctr_x + dx * widths\n",
    "        refined_ctr_y = ctr_y + dy * heights\n",
    "        refined_widths = widths * torch.exp(dw)\n",
    "        refined_heights = heights * torch.exp(dh)\n",
    "        refined_x1 = refined_ctr_x - 0.5 * refined_widths\n",
    "        refined_y1 = refined_ctr_y - 0.5 * refined_heights\n",
    "        refined_x2 = refined_ctr_x + 0.5 * refined_widths\n",
    "        refined_y2 = refined_ctr_y + 0.5 * refined_heights\n",
    "        refined_boxes = torch.stack([refined_x1, refined_y1, refined_x2, refined_y2], dim=1)\n",
    "        \n",
    "        return torch.clamp(refined_boxes, min=0)\n",
    "\n",
    "    def predict(self, features: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        class_scores = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        \n",
    "        return class_scores, bbox_deltas"
   ],
   "id": "e5026cc2ba411299",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:15:46.349621Z",
     "start_time": "2024-10-15T08:15:43.757622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def list_image_paths(directory: str) -> list[str]:\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Resnext50RCNN().to(device)\n",
    "criterion_class = nn.BCEWithLogitsLoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_coco_path = os.path.join(\"data\", \"train\", \"coco_annotations.json\")\n",
    "valid_coco_path = os.path.join(\"data\", \"valid\", \"coco_annotations.json\")\n",
    "train_images_dir = os.path.join(\"data\", \"train_small\", \"images\")\n",
    "valid_images_dir = os.path.join(\"data\", \"valid_small\", \"images\")\n",
    "train_dataset = CrackDataset(\n",
    "    train_coco_path,\n",
    "    train_images_dir\n",
    ")\n",
    "valid_dataset = CrackDataset(\n",
    "    valid_coco_path,\n",
    "    valid_images_dir\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=2, verbose=True, delta=0)\n",
    "num_epochs = 30"
   ],
   "id": "d57468468044cd13",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:15:46.360172Z",
     "start_time": "2024-10-15T08:15:46.350632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def match_proposals_to_ground_truth(\n",
    "        proposals: torch.Tensor, \n",
    "        bboxes: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    if bboxes.size(0) == 0:\n",
    "        return (torch.empty(0, 4, device=proposals.device), \n",
    "                torch.empty(0, 4, device=proposals.device), \n",
    "                torch.zeros(proposals.size(0), dtype=torch.bool, device=proposals.device))\n",
    "    \n",
    "    ious = torchvision.ops.box_iou(proposals, bboxes)\n",
    "    max_ious, best_bbox_idx = ious.max(dim=1)\n",
    "    assigned_labels = max_ious > 0.1\n",
    "    matched_proposals = proposals[assigned_labels]\n",
    "    matched_bboxes = bboxes[best_bbox_idx[assigned_labels]]\n",
    "    \n",
    "    return matched_proposals, torch.clamp(matched_bboxes, min=0), assigned_labels\n",
    "\n",
    "\n",
    "proposal_cache = defaultdict(list)\n",
    "\n",
    "\n",
    "def perform_selective_search(image: np.ndarray, image_path: str, batch_size: int = 70) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform selective search and return the largest proposals (by area) in three batches after shuffling.\n",
    "    Cache the proposals based on the image path to avoid redundant computation.\n",
    "    \n",
    "    Args:\n",
    "    - image: The input image as a numpy array.\n",
    "    - image_path: The file path to the image (used as the cache key).\n",
    "    - batch_size: The size of each batch (default 50).\n",
    "    \n",
    "    Yields:\n",
    "    - A batch of proposals as a torch.Tensor of shape [batch_size, 4].\n",
    "    \"\"\"\n",
    "    if image_path in proposal_cache:\n",
    "        for start in range(0, len(proposal_cache[image_path]), batch_size):\n",
    "            yield torch.tensor(proposal_cache[image_path][start:start + batch_size], dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "        \n",
    "        ss.setBaseImage(image)\n",
    "        ss.switchToSelectiveSearchFast()\n",
    "\n",
    "        rects = ss.process()\n",
    "        boxes = []\n",
    "\n",
    "        for (x, y, w, h) in rects:\n",
    "            area = w * h\n",
    "\n",
    "            boxes.append((x, y, x + w, y + h, area))\n",
    "\n",
    "        boxes = sorted(boxes, key=lambda b: b[4], reverse=True)\n",
    "        boxes = [(x1, y1, x2, y2) for x1, y1, x2, y2, area in boxes]\n",
    "        num_proposals = min(len(boxes), 2 * batch_size)\n",
    "        top_proposals = boxes[:num_proposals]\n",
    "\n",
    "        random.shuffle(top_proposals)\n",
    "\n",
    "        proposal_cache[image_path] = top_proposals\n",
    "\n",
    "        for start in range(0, num_proposals, batch_size):\n",
    "            yield torch.tensor(top_proposals[start:start + batch_size], dtype=torch.float32).to(device)"
   ],
   "id": "b6020603d0ee1225",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:15:46.370003Z",
     "start_time": "2024-10-15T08:15:46.361926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_model(image: torch.Tensor, proposals: torch.Tensor, bboxes: list[int], is_test: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    features = model.extract_region_features(image, proposals)\n",
    "    class_scores, bbox_deltas = model.predict(features) if is_test else model(features)\n",
    "    refined_bboxes = model.refine_bboxes(proposals, bbox_deltas)\n",
    "    bboxes_tensor = torch.tensor(bboxes[i], dtype=torch.float32, device=device)\n",
    "    matched_proposals, matched_bboxes, assigned_labels = match_proposals_to_ground_truth(proposals, bboxes_tensor)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cls_loss = criterion_class(class_scores, assigned_labels.float().unsqueeze(1))\n",
    "\n",
    "    if len(matched_proposals) > 0 and len(refined_bboxes[assigned_labels]) > 0:\n",
    "        bbox_loss = criterion_bbox(refined_bboxes[assigned_labels], matched_bboxes)\n",
    "    else:\n",
    "        bbox_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    return cls_loss, bbox_loss\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, train_cls_losses, train_bbox_losses, val_cls_losses, val_bbox_losses, filepath):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'train_cls_losses': train_cls_losses,\n",
    "        'train_bbox_losses': train_bbox_losses,\n",
    "        'val_cls_losses': val_cls_losses,\n",
    "        'val_bbox_losses': val_bbox_losses\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    if os.path.isfile(filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        train_cls_losses = checkpoint['train_cls_losses']\n",
    "        train_bbox_losses = checkpoint['train_bbox_losses']\n",
    "        val_cls_losses = checkpoint['val_cls_losses']\n",
    "        val_bbox_losses = checkpoint['val_bbox_losses']\n",
    "        \n",
    "        print(f\"Checkpoint loaded from {filepath}\")\n",
    "        \n",
    "        return epoch, train_cls_losses, train_bbox_losses, val_cls_losses, val_bbox_losses\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {filepath}\")\n",
    "        \n",
    "        return 0, [], [], [], []"
   ],
   "id": "191e000c65dc1140",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:16:50.233608Z",
     "start_time": "2024-10-15T08:15:46.372013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_path = os.path.join(\"checkpoints\", f\"rcnn_with_resnext\")\n",
    "start_epoch, train_cls_losses, train_bbox_losses, val_cls_losses, val_bbox_losses = load_checkpoint(checkpoint_path)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    total_cls_loss = 0.0\n",
    "    total_bbox_loss = 0.0\n",
    "    epoch_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for image_paths, images, bboxes in epoch_progress:\n",
    "        images = torch.tensor(np.array(images), dtype=torch.uint8, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            image_path = image_paths[i]\n",
    "            prepared_image = image.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "            for proposals in perform_selective_search(prepared_image, image_path=image_path):\n",
    "                cls_loss, bbox_loss = run_model(image, proposals, bboxes)\n",
    "                loss = cls_loss + bbox_loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_cls_loss += cls_loss.item()\n",
    "                total_bbox_loss += bbox_loss.item()\n",
    "\n",
    "        avg_cls_loss = total_cls_loss / len(train_dataloader)\n",
    "        avg_bbox_loss = total_bbox_loss / len(train_dataloader)\n",
    "\n",
    "        train_cls_losses.append(avg_cls_loss)\n",
    "        train_bbox_losses.append(avg_bbox_loss)\n",
    "        epoch_progress.set_postfix({\n",
    "            \"Cls Loss\": f\"{avg_cls_loss:.4f}\",\n",
    "            \"BBox Loss\": f\"{avg_bbox_loss:.4f}\"\n",
    "        })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Avg Classification Loss: {avg_cls_loss:.4f}, Avg BBox Regression Loss: {avg_bbox_loss:.4f}\")\n",
    "    model.eval()\n",
    "    \n",
    "    val_cls_loss = 0.0\n",
    "    val_bbox_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_paths, images, bboxes in valid_dataloader:\n",
    "            images = torch.tensor(np.array(images), dtype=torch.uint8, device=device)\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "                image_path = image_paths[i]\n",
    "                prepared_image = image.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "                for proposals in perform_selective_search(prepared_image, image_path=image_path):\n",
    "                    cls_loss, bbox_loss = run_model(image, proposals, bboxes)\n",
    "                    val_cls_loss += cls_loss.item()\n",
    "                    val_bbox_loss += bbox_loss.item()\n",
    "\n",
    "    avg_val_cls_loss = val_cls_loss / len(valid_dataloader)\n",
    "    avg_val_bbox_loss = val_bbox_loss / len(valid_dataloader)\n",
    "    avg_val_loss = avg_val_cls_loss + avg_val_bbox_loss\n",
    "\n",
    "    val_cls_losses.append(avg_val_cls_loss)\n",
    "    val_bbox_losses.append(avg_val_bbox_loss)\n",
    "    print(f\"Validation completed. Avg Validation Classification Loss: {avg_val_cls_loss:.4f}, Avg Validation BBox Regression Loss: {avg_val_bbox_loss:.4f}\")\n",
    "    early_stopping(avg_val_loss, model, f\"{checkpoint_path}_{epoch}.pt\")\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    save_checkpoint(model, optimizer, epoch + 1, train_cls_losses, train_bbox_losses, val_cls_losses, val_bbox_losses, checkpoint_path)"
   ],
   "id": "34b517dc2db79428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at checkpoints\\rcnn_with_resnext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 3/3 [00:31<00:00, 10.36s/batch, Cls Loss=627.3776, BBox Loss=560447778.7117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Classification Loss: 627.3776, Avg BBox Regression Loss: 560447778.7117\n",
      "Validation completed. Avg Validation Classification Loss: 29809439.0000, Avg Validation BBox Regression Loss: 2414.8750\n",
      "Validation loss decreased (inf --> 29811853.875000).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30:   0%|          | 0/3 [00:11<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m total_bbox_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m      9\u001B[0m epoch_progress \u001B[38;5;241m=\u001B[39m tqdm(train_dataloader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m image_paths, images, bboxes \u001B[38;5;129;01min\u001B[39;00m epoch_progress:\n\u001B[0;32m     12\u001B[0m     images \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(np\u001B[38;5;241m.\u001B[39marray(images), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39muint8, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m     14\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    438\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 440\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_iterator()\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[1;32m--> 388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _MultiProcessingDataLoaderIter(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m   1031\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1032\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[1;32m-> 1038\u001B[0m w\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\multiprocessing\\process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[0;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    120\u001B[0m _cleanup()\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Popen(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\multiprocessing\\context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_context\u001B[38;5;241m.\u001B[39mget_context()\u001B[38;5;241m.\u001B[39mProcess\u001B[38;5;241m.\u001B[39m_Popen(process_obj)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\multiprocessing\\context.py:336\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[1;34m(process_obj)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_win32\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Popen(process_obj)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, process_obj)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     94\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(prep_data, to_child)\n\u001B[1;32m---> 95\u001B[0m     reduction\u001B[38;5;241m.\u001B[39mdump(process_obj, to_child)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     97\u001B[0m     set_spawning_popen(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\multiprocessing\\reduction.py:60\u001B[0m, in \u001B[0;36mdump\u001B[1;34m(obj, file, protocol)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdump\u001B[39m(obj, file, protocol\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     59\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m     ForkingPickler(file, protocol)\u001B[38;5;241m.\u001B[39mdump(obj)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Resnext50RCNN()\n",
    "checkpoint_path = os.path.join(\"checkpoints\", f\"rcnn_with_resnext_1.pt\")\n",
    "\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "num_images_to_display = 10\n",
    "random_indices = random.sample(range(len(valid_dataset)), num_images_to_display)\n",
    "selected_images = [valid_dataset[idx] for idx in random_indices]\n",
    "results = []\n",
    "\n",
    "for image_path, image, _ in selected_images:\n",
    "    prepared_image = image.cpu().permute(1, 2, 0).numpy()\n",
    "    predictions = []\n",
    "    all_proposals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for proposals in perform_selective_search(prepared_image, image_path=image_path):\n",
    "            features = model.extract_region_features(image.to(device), proposals)\n",
    "            class_scores, bbox_deltas = model.predict(features)\n",
    "            refined_bboxes = model.refine_bboxes(proposals, bbox_deltas)\n",
    "            high_score_indices = class_scores.squeeze() > 0\n",
    "            refined_bboxes = refined_bboxes[high_score_indices]\n",
    "    \n",
    "            predictions.extend(refined_bboxes.cpu().numpy().tolist()[0])\n",
    "            all_proposals.extend(proposals)\n",
    "        \n",
    "    results.append((image_path, image, predictions, all_proposals))\n",
    "\n",
    "print([t[-2] for t in results])\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i, (image_path, image, predictions, proposals) in enumerate(results):\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "\n",
    "    image_with_boxes = image.permute(1, 2, 0).cpu().numpy().copy()\n",
    "\n",
    "    for (x1, y1, x2, y2) in predictions:\n",
    "        cv2.rectangle(image_with_boxes, (int(x1), int(y1)), (int(x2) + 1, int(y2) + 1), (0, 255, 0), 2)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Image: {os.path.basename(image_path)}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ca6fedef37de6453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "csc = class_scores.cpu().numpy()\n",
    "bbd = bbox_deltas.cpu().numpy()"
   ],
   "id": "166787380cc78669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b26d2700dcb048f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crack_detection_torch_1)",
   "language": "python",
   "name": "crack_detection_torch_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
