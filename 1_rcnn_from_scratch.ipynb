{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T07:32:40.858736Z",
     "start_time": "2024-09-07T07:32:40.849899Z"
    }
   },
   "cell_type": "code",
   "source": "%env CUDA_LAUNCH_BLOCKING=1",
   "id": "b8be3afe7139f87c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-07T07:32:44.310100Z",
     "start_time": "2024-09-07T07:32:40.860742Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable\n",
    "\n",
    "from helpers.datasets import CrackDataset, custom_collate_fn\n",
    "from helpers.early_stopping import EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T07:32:44.323841Z",
     "start_time": "2024-09-07T07:32:44.311497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Resnext50RCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 224, 224), linear_layers_features=512):\n",
    "        super(Resnext50RCNN, self).__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 1)\n",
    "        )\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 4)\n",
    "        )\n",
    "        self.roi_size = (input_shape[1], input_shape[2])\n",
    "\n",
    "    def _get_feature_size(self, shape: tuple):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "            return features.numel()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def extract_region_features(self, image: torch.Tensor, boxes: Iterable[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        For each bounding box, crop the region, resize to the input shape, and pass through the feature extractor.\n",
    "        \"\"\"\n",
    "        regions = []\n",
    "        image = image.float() / 255.0\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.int()\n",
    "            region = image[:, y1:y2, x1:x2]\n",
    "            region_resized = nn.functional.interpolate(region.unsqueeze(0), size=self.roi_size, mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            regions.append(region_resized)\n",
    "\n",
    "        regions = torch.cat(regions, dim=0)\n",
    "        features = self.feature_extractor(regions)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def refine_bboxes(self, proposals: torch.Tensor, deltas: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Refines the original bounding boxes (proposals) using the predicted bbox deltas.\n",
    "        Args:\n",
    "        - proposals: The original bounding boxes from selective search (x1, y1, x2, y2)\n",
    "        - deltas: Predicted bounding box adjustments (dx, dy, dw, dh)\n",
    "\n",
    "        Returns:\n",
    "        - Refined bounding boxes (x1', y1', x2', y2')\n",
    "        \"\"\"\n",
    "        widths = proposals[:, 2] - proposals[:, 0]\n",
    "        heights = proposals[:, 3] - proposals[:, 1]\n",
    "        ctr_x = proposals[:, 0] + 0.5 * widths\n",
    "        ctr_y = proposals[:, 1] + 0.5 * heights\n",
    "        dx = deltas[:, 0]\n",
    "        dy = deltas[:, 1]\n",
    "        dw = deltas[:, 2]\n",
    "        dh = deltas[:, 3]\n",
    "        refined_ctr_x = ctr_x + dx * widths\n",
    "        refined_ctr_y = ctr_y + dy * heights\n",
    "        refined_widths = widths * torch.exp(dw)\n",
    "        refined_heights = heights * torch.exp(dh)\n",
    "        refined_x1 = refined_ctr_x - 0.5 * refined_widths\n",
    "        refined_y1 = refined_ctr_y - 0.5 * refined_heights\n",
    "        refined_x2 = refined_ctr_x + 0.5 * refined_widths\n",
    "        refined_y2 = refined_ctr_y + 0.5 * refined_heights\n",
    "        refined_boxes = torch.stack([refined_x1, refined_y1, refined_x2, refined_y2], dim=1)\n",
    "        \n",
    "        return refined_boxes\n",
    "\n",
    "    def predict(self, features: torch.Tensor):\n",
    "        class_scores = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        \n",
    "        return class_scores, bbox_deltas"
   ],
   "id": "e5026cc2ba411299",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T07:32:47.972556Z",
     "start_time": "2024-09-07T07:32:44.325847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def list_image_paths(directory: str) -> list[str]:\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Resnext50RCNN().to(device)\n",
    "criterion_class = nn.BCEWithLogitsLoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_coco_path = os.path.join(\"data\", \"train\", \"coco_annotations.json\")\n",
    "valid_coco_path = os.path.join(\"data\", \"valid\", \"coco_annotations.json\")\n",
    "train_images_dir = os.path.join(\"data\", \"train\", \"images\")\n",
    "valid_images_dir = os.path.join(\"data\", \"valid\", \"images\")\n",
    "train_dataset = CrackDataset(\n",
    "    train_coco_path,\n",
    "    train_images_dir\n",
    ")\n",
    "valid_dataset = CrackDataset(\n",
    "    valid_coco_path,\n",
    "    valid_images_dir\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=7, verbose=True, delta=0)\n",
    "num_epochs = 30"
   ],
   "id": "d57468468044cd13",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def assign_labels_to_proposals(proposals: torch.Tensor, bboxes: np.ndarray, iou_threshold: float = 0.5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Assigns binary labels (1 for foreground, 0 for background) to proposals based on IoU with ground truth boxes.\n",
    "    \n",
    "    Arguments:\n",
    "    - proposals: tensor of proposals, shape [N, 4]\n",
    "    - bboxes: tensor of ground truth boxes, shape [M, 4]\n",
    "    - iou_threshold: IoU threshold for considering a proposal as positive\n",
    "    \n",
    "    Returns:\n",
    "    - assigned_labels: tensor of binary labels for each proposal, shape [N]\n",
    "    \"\"\"\n",
    "    assigned_labels = torch.zeros(proposals.shape[0], dtype=torch.long, device=proposals.device)\n",
    "\n",
    "    for _, proposal in enumerate(proposals):\n",
    "        max_iou = 0\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            iou = torchvision.ops.box_iou(proposal, bbox)\n",
    "\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "\n",
    "        if max_iou >= iou_threshold:\n",
    "            assigned_labels[i] = 1\n",
    "\n",
    "    return assigned_labels\n",
    "\n",
    "\n",
    "def perform_selective_search(image: np.ndarray, max_proposals=5) -> torch.Tensor:\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "\n",
    "    rects = ss.process()\n",
    "    boxes = []\n",
    "\n",
    "    for (x, y, w, h) in rects[:max_proposals]:\n",
    "        boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "    return torch.tensor(boxes, dtype=torch.float32).to(device)"
   ],
   "id": "b6020603d0ee1225"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T07:33:05.051626Z",
     "start_time": "2024-09-07T07:32:47.973591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_cls_loss = 0.0\n",
    "    total_bbox_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for image_paths, images, bboxes in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        for i, image in enumerate(images):\n",
    "            proposals = perform_selective_search(image.cpu().numpy())\n",
    "            features = model.extract_region_features(image.permute(2, 0, 1), proposals)\n",
    "            class_scores, bbox_deltas = model.predict(features)\n",
    "            refined_bboxes = model.refine_bboxes(proposals, bbox_deltas)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            print(class_scores)\n",
    "            print(labels[i].long())\n",
    "            cls_loss = criterion_class(class_scores, labels[i].long())\n",
    "            bbox_loss = criterion_bbox(refined_bboxes, bboxes[i])\n",
    "            loss = cls_loss + bbox_loss\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_cls_loss += cls_loss.item()\n",
    "            total_bbox_loss += bbox_loss.item()\n",
    "    \n",
    "    avg_cls_loss = total_cls_loss / len(train_dataloader)\n",
    "    avg_bbox_loss = total_bbox_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Classification Loss: {avg_cls_loss:.4f}, BBox Regression Loss: {avg_bbox_loss:.4f}\")\n"
   ],
   "id": "34b517dc2db79428",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4875],\n",
      "        [ 0.1486],\n",
      "        [ 0.2978],\n",
      "        [ 0.0817],\n",
      "        [ 0.2505]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([2])) must be the same as input size (torch.Size([5, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 38\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(class_scores)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(labels[i]\u001B[38;5;241m.\u001B[39mlong())\n\u001B[1;32m---> 38\u001B[0m cls_loss \u001B[38;5;241m=\u001B[39m criterion_class(class_scores, labels[i]\u001B[38;5;241m.\u001B[39mlong())\n\u001B[0;32m     39\u001B[0m bbox_loss \u001B[38;5;241m=\u001B[39m criterion_bbox(refined_bboxes, bboxes[i])\n\u001B[0;32m     40\u001B[0m loss \u001B[38;5;241m=\u001B[39m cls_loss \u001B[38;5;241m+\u001B[39m bbox_loss\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:734\u001B[0m, in \u001B[0;36mBCEWithLogitsLoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    733\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\u001B[38;5;28minput\u001B[39m, target,\n\u001B[0;32m    735\u001B[0m                                               \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[0;32m    736\u001B[0m                                               pos_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_weight,\n\u001B[0;32m    737\u001B[0m                                               reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction)\n",
      "File \u001B[1;32m~\\.conda\\envs\\crack_detection_torch_1\\Lib\\site-packages\\torch\\nn\\functional.py:3242\u001B[0m, in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[0;32m   3239\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[0;32m   3241\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[1;32m-> 3242\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) must be the same as input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   3244\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\u001B[38;5;28minput\u001B[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001B[1;31mValueError\u001B[0m: Target size (torch.Size([2])) must be the same as input size (torch.Size([5, 1]))"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b4e4a7e31de2260",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crack_detection_torch_1)",
   "language": "python",
   "name": "crack_detection_torch_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
