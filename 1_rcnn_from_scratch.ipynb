{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:49.563015Z",
     "start_time": "2024-09-07T15:13:49.548816Z"
    }
   },
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable\n",
    "\n",
    "from helpers.datasets import CrackDataset, custom_collate_fn\n",
    "from helpers.early_stopping import EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:49.577552Z",
     "start_time": "2024-09-07T15:13:49.564021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Resnext50RCNN(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 224, 224), linear_layers_features=512):\n",
    "        super(Resnext50RCNN, self).__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 1)\n",
    "        )\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._get_feature_size(input_shape), linear_layers_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(linear_layers_features, 4)\n",
    "        )\n",
    "        self.roi_size = (input_shape[1], input_shape[2])\n",
    "\n",
    "    def _get_feature_size(self, shape: tuple):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            features = self.feature_extractor(dummy_input)\n",
    "            return features.numel()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_extractor(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def extract_region_features(self, image: torch.Tensor, boxes: Iterable[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        For each bounding box, crop the region, resize to the input shape, and pass through the feature extractor.\n",
    "        \"\"\"\n",
    "        regions = []\n",
    "        image = image.float() / 255.0\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.int()\n",
    "            region = image[:, y1:y2, x1:x2]\n",
    "            region_resized = nn.functional.interpolate(region.unsqueeze(0), size=self.roi_size, mode=\"bilinear\", align_corners=False)\n",
    "            \n",
    "            regions.append(region_resized)\n",
    "\n",
    "        regions = torch.cat(regions, dim=0)\n",
    "        features = self.feature_extractor(regions)\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def refine_bboxes(self, proposals: torch.Tensor, deltas: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Refines the original bounding boxes (proposals) using the predicted bbox deltas.\n",
    "        Args:\n",
    "        - proposals: The original bounding boxes from selective search (x1, y1, x2, y2)\n",
    "        - deltas: Predicted bounding box adjustments (dx, dy, dw, dh)\n",
    "\n",
    "        Returns:\n",
    "        - Refined bounding boxes (x1', y1', x2', y2')\n",
    "        \"\"\"\n",
    "        widths = proposals[:, 2] - proposals[:, 0]\n",
    "        heights = proposals[:, 3] - proposals[:, 1]\n",
    "        ctr_x = proposals[:, 0] + 0.5 * widths\n",
    "        ctr_y = proposals[:, 1] + 0.5 * heights\n",
    "        dx = deltas[:, 0]\n",
    "        dy = deltas[:, 1]\n",
    "        dw = deltas[:, 2]\n",
    "        dh = deltas[:, 3]\n",
    "        refined_ctr_x = ctr_x + dx * widths\n",
    "        refined_ctr_y = ctr_y + dy * heights\n",
    "        refined_widths = widths * torch.exp(dw)\n",
    "        refined_heights = heights * torch.exp(dh)\n",
    "        refined_x1 = refined_ctr_x - 0.5 * refined_widths\n",
    "        refined_y1 = refined_ctr_y - 0.5 * refined_heights\n",
    "        refined_x2 = refined_ctr_x + 0.5 * refined_widths\n",
    "        refined_y2 = refined_ctr_y + 0.5 * refined_heights\n",
    "        refined_boxes = torch.stack([refined_x1, refined_y1, refined_x2, refined_y2], dim=1)\n",
    "        \n",
    "        return torch.clamp(refined_boxes, min=0)\n",
    "\n",
    "    def predict(self, features: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        class_scores = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        \n",
    "        return class_scores, bbox_deltas"
   ],
   "id": "e5026cc2ba411299",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:51.720110Z",
     "start_time": "2024-09-07T15:13:49.577552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def list_image_paths(directory: str) -> list[str]:\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Resnext50RCNN().to(device)\n",
    "criterion_class = nn.BCEWithLogitsLoss()\n",
    "criterion_bbox = nn.SmoothL1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_coco_path = os.path.join(\"data\", \"train\", \"coco_annotations.json\")\n",
    "valid_coco_path = os.path.join(\"data\", \"valid\", \"coco_annotations.json\")\n",
    "train_images_dir = os.path.join(\"data\", \"train\", \"images\")\n",
    "valid_images_dir = os.path.join(\"data\", \"valid\", \"images\")\n",
    "train_dataset = CrackDataset(\n",
    "    train_coco_path,\n",
    "    train_images_dir\n",
    ")\n",
    "valid_dataset = CrackDataset(\n",
    "    valid_coco_path,\n",
    "    valid_images_dir\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=7, verbose=True, delta=0)\n",
    "num_epochs = 30"
   ],
   "id": "d57468468044cd13",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T15:13:51.734417Z",
     "start_time": "2024-09-07T15:13:51.720110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def match_proposals_to_ground_truth(\n",
    "        proposals: torch.Tensor, \n",
    "        bboxes: torch.Tensor, \n",
    "        iou_threshold: float = 0.5\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    if bboxes.size(0) == 0:\n",
    "        return (torch.empty(0, 4, device=proposals.device), \n",
    "                torch.empty(0, 4, device=proposals.device), \n",
    "                torch.zeros(proposals.size(0), dtype=torch.bool, device=proposals.device))\n",
    "    \n",
    "    ious = torchvision.ops.box_iou(proposals, bboxes)\n",
    "    max_ious, best_bbox_idx = ious.max(dim=1)\n",
    "    assigned_labels = max_ious >= iou_threshold\n",
    "    matched_proposals = proposals[assigned_labels]\n",
    "    matched_bboxes = bboxes[best_bbox_idx[assigned_labels]]\n",
    "    \n",
    "    return matched_proposals, torch.clamp(matched_bboxes, min=0), assigned_labels\n",
    "\n",
    "\n",
    "def perform_selective_search(image: np.ndarray, batch_size: int = 70) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform selective search and return the largest proposals (by area) in three batches after shuffling.\n",
    "    Args:\n",
    "    - image: The input image as a numpy array.\n",
    "    - batch_size: The size of each batch (default 70).\n",
    "    \n",
    "    Yields:\n",
    "    - A batch of proposals as a torch.Tensor of shape [batch_size, 4].\n",
    "    \"\"\"\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "\n",
    "    rects = ss.process()\n",
    "    boxes = []\n",
    "\n",
    "    for (x, y, w, h) in rects:\n",
    "        area = w * h\n",
    "        \n",
    "        boxes.append((x, y, x + w, y + h, area))\n",
    "\n",
    "    boxes = sorted(boxes, key=lambda b: b[4], reverse=True)\n",
    "    boxes = [(x1, y1, x2, y2) for x1, y1, x2, y2, area in boxes]\n",
    "    num_proposals = min(len(boxes), 3 * batch_size)\n",
    "    top_proposals = boxes[:num_proposals]\n",
    "\n",
    "    random.shuffle(top_proposals)\n",
    "\n",
    "    for start in range(0, num_proposals, batch_size):\n",
    "        end = min(start + batch_size, num_proposals)\n",
    "        \n",
    "        yield torch.tensor(top_proposals[start:end], dtype=torch.float32).to(device)"
   ],
   "id": "b6020603d0ee1225",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T18:57:50.570237Z",
     "start_time": "2024-09-07T15:13:51.734417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_cls_loss = 0.0\n",
    "    total_bbox_loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for image_paths, images, bboxes in epoch_progress:\n",
    "        images = torch.tensor(np.array(images), dtype=torch.uint8, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            for proposals in perform_selective_search(image.cpu().numpy()):\n",
    "                features = model.extract_region_features(image.permute(2, 0, 1), proposals)\n",
    "                class_scores, bbox_deltas = model.predict(features)\n",
    "                refined_bboxes = model.refine_bboxes(proposals, bbox_deltas)\n",
    "                bboxes_tensor = torch.tensor(bboxes[i], dtype=torch.float32, device=device)\n",
    "                matched_proposals, matched_bboxes, assigned_labels = match_proposals_to_ground_truth(proposals, bboxes_tensor)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                cls_loss = criterion_class(class_scores, assigned_labels.float().unsqueeze(1))\n",
    "\n",
    "                if len(matched_proposals) > 0 and len(refined_bboxes[assigned_labels]) > 0:\n",
    "                    bbox_loss = criterion_bbox(refined_bboxes[assigned_labels], matched_bboxes)\n",
    "                else:\n",
    "                    bbox_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                loss = cls_loss + bbox_loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_cls_loss += cls_loss.item()\n",
    "                total_bbox_loss += bbox_loss.item()\n",
    "\n",
    "        avg_cls_loss = total_cls_loss / len(train_dataloader)\n",
    "        avg_bbox_loss = total_bbox_loss / len(train_dataloader)\n",
    "\n",
    "        epoch_progress.set_postfix({\n",
    "            \"Cls Loss\": f\"{avg_cls_loss:.4f}\",\n",
    "            \"BBox Loss\": f\"{avg_bbox_loss:.4f}\"\n",
    "        })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Avg Classification Loss: {avg_cls_loss:.4f}, Avg BBox Regression Loss: {avg_bbox_loss:.4f}\")\n"
   ],
   "id": "34b517dc2db79428",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:  51%|█████     | 151/295 [3:43:58<3:33:35, 88.99s/batch, Cls Loss=508.5064, BBox Loss=30893713.2418]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m         loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     34\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 36\u001B[0m         total_cls_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m cls_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     37\u001B[0m         total_bbox_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m bbox_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     39\u001B[0m avg_cls_loss \u001B[38;5;241m=\u001B[39m total_cls_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ca6fedef37de6453",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crack_detection_torch_1)",
   "language": "python",
   "name": "crack_detection_torch_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
